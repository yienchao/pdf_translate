"""
100% Haiku Translation Test
- NO dictionary lookup
- NO indexed files
- EVERYTHING gets translated by Haiku 4.5
- Purpose: Identify source of translation gaps
"""
import fitz
import os
import sys
import time
from pathlib import Path
from anthropic_translator import translate_batch

# Folders
TRANSLATED_FOLDER = "translated_pdfs"

def should_skip(text):
    """Skip empty, numbers only, units, acronyms, technical codes"""
    if not text or not text.strip():
        return True

    # Skip if no letters (pure numbers, symbols, etc.)
    if not any(c.isalpha() for c in text):
        return True

    # Skip common units and measurement abbreviations
    units = {'MM', 'CM', 'M', 'KG', 'LB', 'FT', 'IN', 'SQ', 'MIN', 'MAX', 'NO', 'QTY', 'TYP', 'REF',
             'GA', 'CAL', 'PSI', 'KPA', 'MPH', 'KPH', 'DEG', 'TEMP', 'DIA', 'THK', 'EA'}
    if text.upper().strip('.,;:()[]{}!?-') in units:
        return True

    # Skip material codes ONLY if they match specific patterns
    if len(text) <= 4 and not text.isupper():
        # Skip only if it has numbers (like "1a", "2b")
        if any(c.isdigit() for c in text):
            return True
        # Skip ONLY known material code patterns
        material_codes = {'ac', 'aci', 'al', 'ar', 'asp', 'bo', 'br', 'bv', 'bz',
                         'ca', 'cc', 'cf', 'cg', 'ci', 'cp', 'cr', 'cs', 'ct', 'cu', 'cv',
                         'ea', 'ec', 'ei', 'pbo', 'pfs', 'pi', 'pla', 'prt', 'ps', 'pt', 'pvb',
                         'rm', 'rv', 'st', 'ta', 'tc', 'te', 'ti', 'tm', 'tn', 'tep', 'tr',
                         'vac', 'vc', 'vcr'}
        if text.lower() in material_codes:
            return True

    # Skip reference codes like "PL1", "MF2", "A-505", "G485"
    import re
    if re.match(r'^[A-Z]{1,3}\d+[a-z]?$', text, re.IGNORECASE):
        return True
    if re.match(r'^[A-Z]-\d+[a-z]?$', text, re.IGNORECASE):
        return True

    return False

def merge_text_spans(spans):
    """Merge adjacent text spans"""
    if not spans:
        return []

    spans.sort(key=lambda s: (round(s["bbox"][1], 1), s["bbox"][0]))

    merged = []
    current = spans[0].copy()
    current["bbox"] = list(current["bbox"])

    for next_span in spans[1:]:
        same_line = abs(current["bbox"][1] - next_span["bbox"][1]) < 2
        x_gap = next_span["bbox"][0] - current["bbox"][2]
        close_horizontal = -1 <= x_gap <= 5

        if same_line and close_horizontal:
            if x_gap > 0.5:
                current["text"] += " " + next_span["text"]
            else:
                current["text"] += next_span["text"]
            current["bbox"][2] = max(current["bbox"][2], next_span["bbox"][2])
        else:
            merged.append(current)
            current = next_span.copy()
            current["bbox"] = list(current["bbox"])

    merged.append(current)
    return merged

def extract_text_from_pdf(pdf_path):
    """Extract all text with positions"""
    doc = fitz.open(pdf_path)
    all_text = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        blocks = page.get_text("dict")["blocks"]

        page_spans = []
        for block in blocks:
            if block.get("type") == 0:
                for line in block.get("lines", []):
                    for span in line.get("spans", []):
                        text = span.get("text", "").strip()
                        if text:
                            page_spans.append({
                                "text": text,
                                "bbox": list(span["bbox"]),
                                "size": span["size"],
                                "color": span.get("color", 0)
                            })

        merged = merge_text_spans(page_spans)

        for item in merged:
            all_text.append({
                "page": page_num,
                "text": item["text"],
                "bbox": item["bbox"],
                "size": item["size"],
                "color": item["color"]
            })

    doc.close()
    return all_text

def process_pdf(input_path, output_path, api_key):
    """Process single PDF with 100% Haiku translation"""
    print(f"\n{'='*80}")
    print(f"100% HAIKU TRANSLATION TEST")
    print(f"Processing: {os.path.basename(input_path)}")
    print('='*80)

    # Extract text
    print("Extracting text...")
    text_elements = extract_text_from_pdf(input_path)
    print(f"   Found {len(text_elements)} text elements")

    # Process each element - EVERYTHING goes to Haiku (no dictionary!)
    needs_translation = {}  # {index: french_text}
    skipped = 0

    for idx, elem in enumerate(text_elements):
        text = elem["text"]

        # Skip only numbers/units
        if should_skip(text):
            elem["translated"] = text
            elem["type"] = "skip"
            skipped += 1
        else:
            # EVERYTHING ELSE ‚Üí Haiku (NO dictionary check!)
            elem["type"] = "needs_haiku"
            needs_translation[str(idx)] = text

    print(f"   Skipped (numbers/units): {skipped}")
    print(f"   Sending to Haiku: {len(needs_translation)}")

    # Translate with Haiku
    input_tokens = 0
    output_tokens = 0

    if needs_translation:
        print(f"\nTranslating {len(needs_translation)} items with Haiku 4.5...")
        try:
            result = translate_batch(needs_translation, api_key, batch_size=100)
            translations = result["translations"]
            input_tokens = result["input_tokens"]
            output_tokens = result["output_tokens"]

            print(f"   Got {len(translations)} translations from Haiku")
            print(f"   Tokens: {input_tokens} input + {output_tokens} output = {input_tokens + output_tokens} total")

            # Apply Haiku translations
            for idx_str, english in translations.items():
                idx_int = int(idx_str)
                if idx_int < len(text_elements):
                    text_elements[idx_int]["translated"] = english
                    text_elements[idx_int]["type"] = "haiku"

        except Exception as e:
            print(f"   Haiku translation failed: {e}")
            return False, 0, 0

    # Count results
    haiku_count = sum(1 for e in text_elements if e.get("type") == "haiku")
    untranslated_count = sum(1 for e in text_elements if "translated" not in e)

    print(f"\n--- TRANSLATION STATS ---")
    print(f"   Total elements: {len(text_elements)}")
    print(f"   Translated by Haiku: {haiku_count}")
    print(f"   Skipped (numbers/units): {skipped}")
    print(f"   UNTRANSLATED (gaps): {untranslated_count}")

    # Show untranslated items if any
    if untranslated_count > 0:
        print(f"\n‚ö†Ô∏è WARNING: {untranslated_count} items were NOT translated!")
        print("These are the GAPS we're investigating:")
        for idx, elem in enumerate(text_elements):
            if "translated" not in elem:
                print(f"   - '{elem['text']}'")
                if idx >= 10:
                    print(f"   ... and {untranslated_count - 10} more")
                    break

    # Apply to PDF
    print("\nApplying translations to PDF...")
    doc = fitz.open(input_path)

    for page_num in range(len(doc)):
        page = doc[page_num]
        page_elements = [e for e in text_elements if e["page"] == page_num]

        # Cover original text with white rectangles
        for elem in page_elements:
            bbox = elem["bbox"]
            rect = fitz.Rect(bbox[0] - 1, bbox[1] - 1, bbox[2] + 1, bbox[3] + 1)
            page.draw_rect(rect, color=(1, 1, 1), fill=(1, 1, 1))

        # Insert translated text
        try:
            font = fitz.Font("helv")
        except:
            font = fitz.Font()

        success_count = 0
        for elem in page_elements:
            translated = elem.get("translated", elem["text"])
            bbox = elem["bbox"]
            size = elem["size"]

            if not translated:
                continue

            # Color conversion
            color_int = elem["color"]
            color = (
                ((color_int >> 16) & 0xFF) / 255.0,
                ((color_int >> 8) & 0xFF) / 255.0,
                (color_int & 0xFF) / 255.0
            )

            try:
                page.insert_text(
                    (bbox[0], bbox[3] - 1),
                    translated,
                    fontsize=size,
                    color=color,
                    render_mode=0
                )
                success_count += 1
            except:
                pass

        if page_num == 0:
            print(f"   Inserted {success_count}/{len(page_elements)} texts on page 1")

    print(f"\nSaving to: {output_path}")
    doc.save(output_path, garbage=4, deflate=True, clean=True)
    doc.close()
    print("Done!")
    return True, input_tokens, output_tokens

def main():
    """Process PDF with 100% Haiku translation"""
    # Get API key
    api_key = os.environ.get("ANTHROPIC_API_KEY")
    if not api_key:
        print("Error: ANTHROPIC_API_KEY environment variable not set")
        print("Set it using: set ANTHROPIC_API_KEY=your-key-here")
        sys.exit(1)

    os.makedirs(TRANSLATED_FOLDER, exist_ok=True)

    # Get files to process
    if len(sys.argv) > 1:
        # Single file mode
        pdf_files = [Path(sys.argv[1])]
    else:
        # Process all PDFs in original folder
        pdf_files = list(Path("original").glob("*.pdf"))

    if not pdf_files:
        print("Error: No PDF files found")
        sys.exit(1)

    print(f"\n{'='*80}")
    print("100% HAIKU TRANSLATION TEST")
    print("Purpose: Identify source of translation gaps")
    print("Method: Translate EVERYTHING with Haiku (no dictionary, no indexed files)")
    print(f"Found {len(pdf_files)} PDF(s) to process")
    print('='*80)

    start_time = time.time()
    success_count = 0
    total_input_tokens = 0
    total_output_tokens = 0

    for idx, pdf_path in enumerate(pdf_files, 1):
        # Show progress with timer
        elapsed = time.time() - start_time
        print(f"\n[{idx}/{len(pdf_files)}] Processing: {pdf_path.name}")
        print(f"‚è±Ô∏è  Elapsed time: {elapsed:.1f}s")

        if not pdf_path.exists():
            print(f"Error: File not found: {pdf_path}")
            continue

        # Create output filename
        output_name = pdf_path.stem + " - HAIKU100TEST.pdf"
        output_path = os.path.join(TRANSLATED_FOLDER, output_name)

        success, input_tokens, output_tokens = process_pdf(str(pdf_path), output_path, api_key)
        if success:
            success_count += 1
            total_input_tokens += input_tokens
            total_output_tokens += output_tokens

    # Final summary
    total_time = time.time() - start_time
    minutes = int(total_time // 60)
    seconds = total_time % 60

    if minutes > 0:
        time_str = f"{minutes}m {seconds:.1f}s"
    else:
        time_str = f"{seconds:.1f}s"

    # Calculate cost (Haiku 4.5 pricing: $0.80/1M input, $4.00/1M output)
    total_tokens = total_input_tokens + total_output_tokens
    cost_input = (total_input_tokens / 1_000_000) * 0.80
    cost_output = (total_output_tokens / 1_000_000) * 4.00
    total_cost = cost_input + cost_output

    print(f"\n{'='*80}")
    print(f"ALL TESTS COMPLETE! Successfully processed {success_count}/{len(pdf_files)} PDFs")
    print(f"‚è±Ô∏è  Total time: {time_str}")
    print(f"\nüìä TOKEN USAGE:")
    print(f"   Input tokens:  {total_input_tokens:,}")
    print(f"   Output tokens: {total_output_tokens:,}")
    print(f"   Total tokens:  {total_tokens:,}")
    print(f"\nüí∞ ESTIMATED COST:")
    print(f"   Input:  ${cost_input:.4f}")
    print(f"   Output: ${cost_output:.4f}")
    print(f"   TOTAL:  ${total_cost:.4f} USD")
    print(f"\nOutput folder: {TRANSLATED_FOLDER}")
    print("\nNext step: Check the PDFs for any remaining French words")
    print("If there are NO gaps ‚Üí Problem was missing dictionary/indexed entries")
    print("If there ARE gaps ‚Üí Problem is PDF text extraction")
    print('='*80)

if __name__ == "__main__":
    main()
